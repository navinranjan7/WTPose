12/22 03:33:02 - mmengine - WARNING - Unsupported operator aten::add_ encountered 3 time(s)
12/22 03:33:02 - mmengine - WARNING - Unsupported operator aten::mul encountered 97 time(s)
12/22 03:33:02 - mmengine - WARNING - Unsupported operator aten::rsub encountered 48 time(s)
12/22 03:33:02 - mmengine - WARNING - Unsupported operator aten::div encountered 210 time(s)
12/22 03:33:02 - mmengine - WARNING - Unsupported operator aten::norm encountered 48 time(s)
12/22 03:33:02 - mmengine - WARNING - Unsupported operator aten::clamp_min encountered 48 time(s)
12/22 03:33:02 - mmengine - WARNING - Unsupported operator aten::expand_as encountered 48 time(s)
12/22 03:33:02 - mmengine - WARNING - Unsupported operator aten::exp encountered 24 time(s)
12/22 03:33:02 - mmengine - WARNING - Unsupported operator aten::sigmoid encountered 24 time(s)
12/22 03:33:02 - mmengine - WARNING - Unsupported operator aten::add encountered 112 time(s)
12/22 03:33:02 - mmengine - WARNING - Unsupported operator aten::softmax encountered 32 time(s)
12/22 03:33:02 - mmengine - WARNING - Unsupported operator aten::gelu encountered 32 time(s)
12/22 03:33:02 - mmengine - WARNING - Unsupported operator aten::fill_ encountered 108 time(s)
12/22 03:33:02 - mmengine - WARNING - Unsupported operator aten::sub encountered 24 time(s)
12/22 03:33:02 - mmengine - WARNING - Unsupported operator aten::ne encountered 12 time(s)
12/22 03:33:02 - mmengine - WARNING - Unsupported operator aten::im2col encountered 3 time(s)
12/22 03:33:02 - mmengine - WARNING - Unsupported operator prim::PythonOp.NATTEN2DQKRPBFunction encountered 8 time(s)
12/22 03:33:02 - mmengine - WARNING - Unsupported operator prim::PythonOp.NATTEN2DAVFunction encountered 8 time(s)
12/22 03:33:02 - mmengine - WARNING - The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.drop_after_pos, backbone.resnet.maxpool, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.attn.w_msa.qkv, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.attn.w_msa.qkv, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.attn.w_msa.qkv, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.attn.w_msa.qkv, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.attn.w_msa.qkv, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.attn.w_msa.qkv, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.10.attn.drop, backbone.stages.2.blocks.10.attn.w_msa.qkv, backbone.stages.2.blocks.10.ffn.dropout_layer, backbone.stages.2.blocks.11.attn.drop, backbone.stages.2.blocks.11.attn.w_msa.qkv, backbone.stages.2.blocks.11.ffn.dropout_layer, backbone.stages.2.blocks.12.attn.drop, backbone.stages.2.blocks.12.attn.w_msa.qkv, backbone.stages.2.blocks.12.ffn.dropout_layer, backbone.stages.2.blocks.13.attn.drop, backbone.stages.2.blocks.13.attn.w_msa.qkv, backbone.stages.2.blocks.13.ffn.dropout_layer, backbone.stages.2.blocks.14.attn.drop, backbone.stages.2.blocks.14.attn.w_msa.qkv, backbone.stages.2.blocks.14.ffn.dropout_layer, backbone.stages.2.blocks.15.attn.drop, backbone.stages.2.blocks.15.attn.w_msa.qkv, backbone.stages.2.blocks.15.ffn.dropout_layer, backbone.stages.2.blocks.16.attn.drop, backbone.stages.2.blocks.16.attn.w_msa.qkv, backbone.stages.2.blocks.16.ffn.dropout_layer, backbone.stages.2.blocks.17.attn.drop, backbone.stages.2.blocks.17.attn.w_msa.qkv, backbone.stages.2.blocks.17.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.attn.w_msa.qkv, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.attn.w_msa.qkv, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.attn.w_msa.qkv, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.attn.w_msa.qkv, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.blocks.6.attn.drop, backbone.stages.2.blocks.6.attn.w_msa.qkv, backbone.stages.2.blocks.6.ffn.dropout_layer, backbone.stages.2.blocks.7.attn.drop, backbone.stages.2.blocks.7.attn.w_msa.qkv, backbone.stages.2.blocks.7.ffn.dropout_layer, backbone.stages.2.blocks.8.attn.drop, backbone.stages.2.blocks.8.attn.w_msa.qkv, backbone.stages.2.blocks.8.ffn.dropout_layer, backbone.stages.2.blocks.9.attn.drop, backbone.stages.2.blocks.9.attn.w_msa.qkv, backbone.stages.2.blocks.9.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.attn.w_msa.qkv, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.attn.w_msa.qkv, backbone.stages.3.blocks.1.ffn.dropout_layer, backbone.stages.3.downsample.adaptive_padding, backbone.wtm.wtm1_0.drop_path, backbone.wtm.wtm1_1.drop_path, backbone.wtm.wtm2_0.drop_path, backbone.wtm.wtm2_1.drop_path, backbone.wtm.wtm3_0.drop_path, backbone.wtm.wtm3_1.drop_path, backbone.wtm.wtm4_0.drop_path, backbone.wtm.wtm4_1.drop_path, data_preprocessor, head.loss_module
12/22 03:33:04 - mmengine - WARNING - Unsupported operator aten::batch_norm encountered 19 time(s)
12/22 03:33:05 - mmengine - WARNING - Unsupported operator aten::layer_norm encountered 71 time(s)
12/22 03:33:05 - mmengine - WARNING - Unsupported operator aten::upsample_bilinear2d encountered 4 time(s)
12/22 03:33:05 - mmengine - WARNING - Unsupported operator aten::adaptive_avg_pool2d encountered 1 time(s)
==============================
Input shape: (1, 3, 384, 288)
Flops: 56.244G
Params: 89.463M
==============================

+---------------------------+----------------------+------------+--------------+
| module                    | #parameters or shape | #flops     | #activations |
+---------------------------+----------------------+------------+--------------+
| model                     | 89.463M              | 56.244G    | 0.178G       |
|  backbone                 |  89.461M             |  56.229G   |  0.178G      |
|   backbone.stages         |   86.885M            |   38.575G  |   98.004M    |
|    backbone.stages.0.blo… |    0.403M            |    2.963G  |    21.466M   |
|    backbone.stages.1      |    1.722M            |    3.184G  |    11.981M   |
|    backbone.stages.2      |    57.434M           |    28.163G |    59.425M   |
|    backbone.stages.3      |    27.326M           |    4.264G  |    5.131M    |
|   backbone.norm0          |   0.256K             |   4.424M   |   0          |
|    backbone.norm0.weight  |    (128,)            |            |              |
|    backbone.norm0.bias    |    (128,)            |            |              |
|   backbone.norm1          |   0.512K             |   2.212M   |   0          |
|    backbone.norm1.weight  |    (256,)            |            |              |
|    backbone.norm1.bias    |    (256,)            |            |              |
|   backbone.norm2          |   1.024K             |   1.106M   |   0          |
|    backbone.norm2.weight  |    (512,)            |            |              |
|    backbone.norm2.bias    |    (512,)            |            |              |
|   backbone.norm3          |   2.048K             |   0.553M   |   0          |
|    backbone.norm3.weight  |    (1024,)           |            |              |
|    backbone.norm3.bias    |    (1024,)           |            |              |
|   backbone.resnet         |   0.255M             |   1.799G   |   11.944M    |
|    backbone.resnet.stem   |    38.848K           |    0.307G  |    2.212M    |
|    backbone.resnet.layer1 |    0.216M            |    1.492G  |    9.732M    |
|   backbone.wtm            |   2.284M             |   15.619G  |   67.461M    |
|    backbone.wtm.wtm1_0    |    0.2M              |    1.368G  |    7.963M    |
|    backbone.wtm.wtm1_1    |    0.2M              |    1.368G  |    7.963M    |
|    backbone.wtm.wtm2_0    |    0.2M              |    1.368G  |    7.963M    |
|    backbone.wtm.wtm2_1    |    0.2M              |    1.368G  |    7.963M    |
|    backbone.wtm.wtm3_0    |    0.2M              |    1.368G  |    7.963M    |
|    backbone.wtm.wtm3_1    |    0.2M              |    1.368G  |    7.963M    |
|    backbone.wtm.wtm4_0    |    0.2M              |    1.368G  |    7.963M    |
|    backbone.wtm.wtm4_1    |    0.2M              |    1.368G  |    7.963M    |
|    backbone.wtm.global_a… |    16.64K            |    0.901M  |    0.128K    |
|    backbone.wtm.conv1     |    0.246M            |    1.699G  |    0.885M    |
|    backbone.wtm.bn1       |    0.256K            |    1.769M  |    0         |
|    backbone.wtm.conv2     |    81.92K            |    0.566G  |    0.885M    |
|    backbone.wtm.bn2       |    0.256K            |    1.769M  |    0         |
|    backbone.wtm.low       |    8.192K            |    56.623M |    0.221M    |
|    backbone.wtm.bn_low    |    64                |    0.442M  |    0         |
|    backbone.wtm.last_conv |    0.332M            |    2.297G  |    1.769M    |
|   backbone.conv3          |   32.768K            |   0.226G   |   0.885M     |
|    backbone.conv3.weight  |    (128, 256, 1, 1)  |            |              |
|   backbone.bn3            |   0.256K             |   1.769M   |   0          |
|    backbone.bn3.weight    |    (128,)            |            |              |
|    backbone.bn3.bias      |    (128,)            |            |              |
|  head.final_layer         |  2.193K              |  15.041M   |  0.118M      |
|   head.final_layer.weight |   (17, 128, 1, 1)    |            |              |
|   head.final_layer.bias   |   (17,)              |            |              |
+---------------------------+----------------------+------------+--------------+

!!!Please be cautious if you use the results in papers. You may need to check if all ops are supported and verify that the flops computation is correct.
